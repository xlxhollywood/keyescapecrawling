# 🕸 방탈출 예약 데이터 크롤러 & 인프라 운영 시스템

> **방탈출 예약/리뷰/동행 플랫폼의 크롤링 및 인프라 전용 레포지토리**

* **기간**: 2024.12.01 \~ 2025.04.20
* **기여도**: 크롤링/배포/운영 시스템 설계 및 구축 100%
* **시연 영상**: [📽 구글 드라이브 링크](https://drive.google.com/drive/folders/1C0baog9rQ4LC-XmpKbN3uXVEPXcWIz9O)


---

## 📌 프로젝트 개요

전국 13개 방탈출 플랫폼의 예약 가능 시간 정보를 실시간으로 수집하고, 사용자에게 최신 데이터를 제공하기 위한 크롤링 어플리케이션 및 인프라 아키텍처를 구축했습니다. 단순 수집을 넘어서 **비동기 스트리밍 구조 + 대용량 데이터 관리 + 자동 운영 환경**을 통합하여 안정적이고 확장 가능한 시스템을 완성했습니다.

---

## 🛠 시스템 아키텍처 주요 구성

### ✅ 웹 크롤링 시스템

* 13개 방탈출 웹사이트 대상 **크롤링 어플리케이션 13종 개발**
* 각 사이트별로 **독립 Docker 컨테이너 구성**, 병렬 실행으로 리소스 분산
* **크롤링 주기**: 기존 평균 2시간 → **10분 주기 수집으로 단축**
* Selenium 기반 데이터 수집 + JSON 구조 정제

### ✅ AWS 기반 배포 / 운영 환경

* **EC2 인스턴스 기반 운영**, Docker Compose로 다중 컨테이너 관리 자동화
* **스팟 인스턴스 + Auto Scaling Group(ASG)** 구성 → **61% 비용 절감**, 무중단 운영 설계

### ✅ Kafka 기반 비동기 구조

* 크롤링 데이터 → **Kafka Topic 전송** → **Kafka Consumer에서 MongoDB 저장**
* MongoDB에는 **테마명 + 날짜 기준 upsert(덮어쓰기)** 적용
* TTL 인덱스로 24시간이 지난 데이터는 **자동 삭제**
* Kafka 로그 보존 기간: 3일
* 실시간 테마 분석 시스템으로의 확장성 고려
* **일일 26만 건 이상 수집 가능 구조 설계 (1800건/10분 × 6 × 24)**

### ✅ GitHub Actions 기반 CI/CD 구축

* 코드 Push 시 자동 빌드 및 무중단 배포 파이프라인 구성

---

## 💡 사용 기술 스택

### 📦 크롤링

* Java, Selenium, BeautifulSoup
* JSON 정제, 예약 데이터 파싱

### 🚀 인프라 / 운영

* **Docker, Docker Compose**
* **AWS EC2, Auto Scaling Group, Spot Instance**
* **GitHub Actions (CI/CD)**

### 🔗 메시징 & DB

* **Apache Kafka** (Producer/Consumer 구조)
* **MongoDB** (TTL 인덱스, Upsert 적용)

---

## 👨‍💻 담당 역할

* 13개 크롤러 개발 및 구조 설계
* Kafka 기반 비동기 아키텍처 설계 및 적용
* AWS 기반 배포/운영 환경 자동화 (Docker, CI/CD)
* 데이터 보존 정책 설계 및 자동 삭제 구조 구축

---

## 🔗 기타

> 본 레포지토리는 메인 서비스 백엔드와 연계되어 **예약 정보의 최신성**, **운영 비용 절감**, **데이터 수집 안정성**을 실현한 핵심 인프라입니다.

---

